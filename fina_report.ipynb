{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Thông tin nhóm: Nhóm 04**\n",
    "\n",
    "**20120083 - Nguyễn Trọng Hiếu**\n",
    "\n",
    "**20120105 - Lê Hoàng Huy**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Application description\n",
    "**What is your chosen application?**\n",
    "\n",
    "*_Thực hiện và tối ưu hóa forward-pass của convolutional layer bằng CUDA._*\n",
    "\n",
    "<li> Fashion MNIST </li>\n",
    "\n",
    "* Input: Ảnh được lưu dưới dạng Eigen::Matrix, số lượng cột là số lượng ảnh, số lượng hàng là thông tin của một ảnh\n",
    "\n",
    "<div style=\"display: flex\">\n",
    "\n",
    "  <div style=\"width: 30%; text-align: center;\">\n",
    "\n",
    "| image 0 | image 1 | image 2 |\n",
    "|:-------:|:-------:|:-------:|\n",
    "| 1<br>6<br>2<br>1<br>2<br>5<br>6<br>0<br>4 | 3<br>1<br>6<br>2<br>8<br>9<br>7<br>4<br>0 | 7<br>1<br>1<br>4<br>1<br>8<br>5<br>0<br>7 |\n",
    "  </div>\n",
    "  <div style=\"width: 30%; color = black; text-align: center;\">\n",
    "\n",
    "*Image1*\n",
    "\n",
    "        1 1 6\n",
    "        6 2 0\n",
    "        2 5 4\n",
    "  </div>\n",
    "  <div style=\"width: 30%; color = black; text-align: center;\">\n",
    "  \n",
    "*Matrix input*\n",
    "\n",
    "        1 1 6\n",
    "        6 2 0\n",
    "        2 5 4\n",
    "        1 2 4\n",
    "        2 8 1\n",
    "        5 9 8\n",
    "        6 7 5\n",
    "        0 4 0\n",
    "        4 0 7\n",
    "  </div>\n",
    "</div\n",
    "<br>\n",
    "\n",
    "*<li>* convolutional layer</li>\n",
    "\n",
    "* Input: Một batch các ảnh dưới dạng Matrix, số lượng cột là số lượng ảnh trong batch, số lượng hàng là thông tin của một ảnh được thể nối các giá trị theo cột như mô tả bên trên.\n",
    "\n",
    "* Output: Số lượng ảnh đầu ra với số hàng là số lượng ảnh và số cột là height_out*width_out\n",
    "\n",
    "* Use cases: khó để lấy vị dụ ảnh thông qua C/C++\n",
    "\n",
    "**Does it need to speed up? Why?**\n",
    "\n",
    "Do thời gian chạy trên cpu lâu và có khả năng song song hóa trên forward-pass của convolutional layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Sequential implementation\n",
    "**Design: Describe steps to go from input to output (don’t show code)**\n",
    "\n",
    "Dựa vào mô hình base <a href='https://github.com/iamhankai/mini-dnn-cpp'>mini-dnn-cpp</a> tiến hành \n",
    "* Xây dựng kiến trúc <a href='https://www.kaggle.com/code/blurredmachine/lenet-architecture-a-complete-guide'>LeNet-5</a>\n",
    "* Chạy trên bộ dữ liệu <a href='https://github.com/zalandoresearch/fashion-mnist'>Fashion MNIST</a>\n",
    "* Tiến hành chạy thử với <a href='https://colab.research.google.com/drive/1pwaG9S3LJyYteGH4P4GQD8R0wLte-BNy?authuser=1#scrollTo=uL24K0K1_m88'>cpu trên  google-colab</a>\n",
    "\n",
    "**Evaluate:**\n",
    "* Describe your experiment setup\n",
    "\n",
    "*_Cần tiến hành tính toán lại channel in/out và kích thước cho từng in/out ở mỗi layer trong mô hình base <a href='https://github.com/iamhankai/mini-dnn-cpp'>mini-dnn-cpp</a>._* <br>\n",
    "\n",
    "*_Tải bộ dữ liệu fashion-MNIST và điều hướng đúng đường dẫn để có thể đọc với thiết lập sẵn có của mô hình base._*\n",
    "\n",
    "* Run the code to see results?\n",
    "\n",
    "*_Kết quả được thể hiện <a href='https://github.com/trhieung/CSC14120/blob/main/report_run.ipynb'>report</a>_*\n",
    "\n",
    "* Does it run correctly?\n",
    "\n",
    "*_Kết quả chạy đúng mong muốn của thiết lập ban đầu._*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Parallel implementation\n",
    "**Analyze: Which steps do you parallelize? Why these steps?**\n",
    "\n",
    "*_Thực hiện song song trên forward-pass của convolutional layer.\n",
    "Nhận xét tính khả thi trong việc tận dụng tính toán song song của GPU để tiết kiệm thời gian._*\n",
    "\n",
    "**Design: How do you parallelize? (don’t show code)**\n",
    "\n",
    "*_Đánh giá về khả năng song song, nhận thấy có thể song song ở những pha sau:_*\n",
    "* pha nhân image_channel với kernel tương ứng, im2col(image.col(i), data_col), đầu ra là một ma trận data_col.\n",
    "* pha nhân 2 ma trận: matmul(data_col, weight)\n",
    "* pha cộng bias: sau khi check size của vector bias, nhận thấy không thích hợp cho việc tiến hành song song (size = 6)\n",
    "* tiến hành song song hóa toàn bộ batch được đưa vào trong forward function.\n",
    "\n",
    "*_Lựa chọn song song của nhóm: Thực hiện 2 khả năng đầu trong 4 khả năng nêu trên_*\n",
    "\n",
    "**Evaluate:**\n",
    "\n",
    "* Describe your experiment setup\n",
    "\n",
    "*_Với mục tiêu tận dụng project sẵn có và tạo ra một sản phẩm có thể ứng dụng và cải tiến. Nhóm đã tiến hành chỉnh sửa một số thông tin trên mô hình base <a href='https://github.com/iamhankai/mini-dnn-cpp'>mini-dnn-cpp</a> tận dụng được nguồi tài nguyên miễn phí google-colab._*\n",
    "\n",
    "*_Tiến hành chỉnh sửa một số thông tin của CMakeList nhằm hỗ trợ run cuda trong project hiện tại. Các hàm hỗ trợ có thể được tạo mới dưới định .cu và .cuh tương ứng trong src/kernel. Ở các hàm trong file .cc khi sử dụng chỉ cần include .cuh tương ứng với .cu (nơi implement cuda code)_*\n",
    "\n",
    "*_Tiến hành run project với <a href='https://github.com/trhieung/CSC14120/blob/main/report_run.ipynb'>template</a>_*\n",
    "* Run the code to see results\n",
    "\n",
    "*_Kết quả được thể hiện <a href='https://github.com/trhieung/CSC14120/blob/main/report_run.ipynb'>report</a>_*\n",
    "\n",
    "* Does it run correctly & faster? If not, do you know why?\n",
    "\n",
    "*_Kết quả chạy đúng khi so sánh ngược lại với cpu. Đảm bảo được time chạy nhanh hơn cpu nhưng không đáng kể (nhanh hơn khoảng <a href='https://github.com/trhieung/CSC14120/blob/main/report_run.ipynb'>2.2 lần</a> tính cho toàn bộ 1 batch)_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Parallel implementation + optimization\n",
    "#### 4.1 pha nhân image_channel với kernel tương ứng\n",
    "**Version 1** <a href='https://github.com/trhieung/CSC14120/blob/main/mini-cnn-cpp-gpu/src/kernel/final.cu'>click here - the comment code</a>\n",
    "\n",
    "**Analyze: Which parts (often: which kernels) do you optimize? Why these parts?**\n",
    "\n",
    "*_Đây là một trong những bước có thể tiến hành song song đã được xem xét và liệt kê ra được_*\n",
    "\n",
    "**Design: How do you optimize? (don’t show code)**\n",
    "*_với điểm ảnh đầu vào, xác định các điểm ảnh đầu ra, với số thread = số điểm ảnh đầu ra trên 1 channel, khi đó mỗi thread sẽ tính toán channel_out điểm ảnh._*\n",
    "\n",
    "*_VD với <a href='https://www.kaggle.com/code/blurredmachine/lenet-architecture-a-complete-guide'>LeNet-5</a>, ở lớp convolution thứ 2 có số channel_out = 16, khi đó ta sẽ có 10x10 threads tính toán song song, mỗi thread tính toán 16 giá trị output._*\n",
    "\n",
    "**Evaluate:**\n",
    "\n",
    "* Describe your experiment setup\n",
    "\n",
    "*_Tiến hành song song hóa dựa trên code được implement sẵn trên host_*\n",
    "* Run the code to see results\n",
    "\n",
    "*_Có viết sẵn hàm test <a href='https://github.com/trhieung/CSC14120/blob/main/mini-cnn-cpp/src/layer/conv.cc'>trong phần comment cuối file</a>_*\n",
    "* Does it run correctly & faster? If not, do you know why?\n",
    "\n",
    "*_Thời gian chạy gần như song song với cpu, nguyên do là ít thread song song, thời gian cho việc chuyển đổi kiểu dữ liệu, khởi tạo, copy từ hosttodevice và ngược lại chiếm phần lớn_*\n",
    "\n",
    "**Version 2** <a href='https://github.com/trhieung/CSC14120/blob/main/mini-cnn-cpp-gpu/src/kernel/final.cu'>click here - the uncomment code</a>\n",
    "\n",
    "**Analyze: Which parts (often: which kernels) do you optimize? Why these parts?**\n",
    "\n",
    "*_Đây là một trong những bước có thể tiến hành song song đã được xem xét và liệt kê ra được_*\n",
    "**Design: How do you optimize? (don’t show code)**\n",
    "\n",
    "*_Tiến hành cải tiến so với version1, mỗi thread lúc này sẽ thực hiện tính toán trên một giá trị đầu ra duy nhất của output._*\n",
    "\n",
    "*_VD với <a href='https://www.kaggle.com/code/blurredmachine/lenet-architecture-a-complete-guide'>LeNet-5</a>, ở lớp convolution thứ 2 có số channel_out = 16, khi đó ta sẽ có 16x10x10 threads tính toán song song, mỗi thread tính toán 1 giá trị output._*\n",
    "**Evaluate:**\n",
    "\n",
    "* Describe your experiment setup\n",
    "\n",
    "*_Tiến hành song song hóa dựa trên code được implement sẵn trên host_*\n",
    "* Run the code to see results\n",
    "\n",
    "*_Có viết sẵn hàm test <a href='https://github.com/trhieung/CSC14120/blob/main/mini-cnn-cpp/src/layer/conv.cc'>trong phần comment cuối file</a>_*\n",
    "* Does it run correctly & faster? If not, do you know why?\n",
    "\n",
    "*_Nhanh hơn rõ rệt, tầm hơn 3 lần (có thể đặt các mốc thời gian start-stop để kiểm tra!_*\n",
    "#### 4.2 pha nhân 2 ma trận: matmul(data_col, weight)\n",
    "**Version 1** \n",
    "\n",
    "**Analyze: Which parts (often: which kernels) do you optimize? Why these parts?**\n",
    "\n",
    "**Design: How do you optimize? (don’t show code)**\n",
    "\n",
    "Tiến hành tận dụng lại code của lab 2 - HW2_P2\n",
    "\n",
    "**Evaluate:**\n",
    "\n",
    "* Describe your experiment setup\n",
    "\n",
    "*_Tiến hành song song hóa dựa trên code được implement sẵn trên host_*\n",
    "* Run the code to see results\n",
    "\n",
    "*_Có viết sẵn hàm test <a href='https://github.com/trhieung/CSC14120/blob/main/mini-cnn-cpp/src/layer/conv.cc'>trong phần comment cuối file</a>_*\n",
    "* Does it run correctly & faster? If not, do you know why?\n",
    "\n",
    "*_Kết quả chạy đúng nhưng nhanh hơn không đáng kể do kích thước ma trận khá nhỏ_*\n",
    "\n",
    "**Version 2**\n",
    "\n",
    "**Analyze: Which parts (often: which kernels) do you optimize? Why these parts?**\n",
    "\n",
    "**Design: How do you optimize? (don’t show code)**\n",
    "\n",
    "Tiến hành tận dụng lại code của lab 2 - HW2_P2 sử dụng TILE_MATMUL\n",
    "\n",
    "**Evaluate:**\n",
    "\n",
    "* Describe your experiment setup\n",
    "\n",
    "*_Tiến hành song song hóa dựa trên code được implement sẵn trên host_*\n",
    "* Run the code to see results\n",
    "\n",
    "*_Có viết sẵn hàm test <a href='https://github.com/trhieung/CSC14120/blob/main/mini-cnn-cpp/src/layer/conv.cc'>trong phần comment cuối file</a>_*\n",
    "* Does it run correctly & faster? If not, do you know why?\n",
    "\n",
    "*_Kết quả chạy đúng nhưng nhanh hơn không đáng kể do kích thước ma trận khá nhỏ_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: \n",
    "* The Eigen::Matrix \"image\" input is configured to store columns concatenated with the columns of a matrix. \n",
    "* The kernel implementation is designed to format the matrix input for row concatenation with rows (**_correct input_**).\n",
    "\n",
    "|Phase|CPU|GPU <br> (Perform calculations exclusively on the optimal versio) <br> (Version 2 in each case)|\n",
    "|:----:|:----:|:----:|\n",
    "|Matmul with kernel <br> (the measure is caculate on one batch - 128 inputs) <br> (2 convolution layers/batch)| **3.583224** <br> (ms/operator)<br><br><br>| **1.145822** <br> (ms/operator) <br> (without including time converting from Eigen::Matrix to correct input) <br> (including the time copy data from cpu to gpu) |\n",
    "|Matmul with Weight matrix <br> (the measure is caculate on one batch - 128 inputs) <br> (2 convolution layers/batch) | **1.033322** <br> (ms/operator)<br><br><br>| **0.297709** <br> (ms/operator) <br> (without including time converting from Eigen::Matrix to correct input) <br> (including the time copy data from cpu to gpu)  |\n",
    "|Total optimise <br> (the measure is caculate on 50 batchs - 128 inputs/batch) <br> (2 convolution layers/batch) | **540.092310** <br> (ms/batch) <br> <br><br>| **244.952008** <br> (ms/batch) <br> (including time converting from Eigen::Matrix to correct input) <br> (including the time copy data from cpu to gpu)  |\n",
    "\n",
    "**_[Reference link](https://colab.research.google.com/drive/1pwaG9S3LJyYteGH4P4GQD8R0wLte-BNy?authuser=3#scrollTo=pPUN-zT8OXmY)_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Reflection\n",
    "* Eachmember: What difficulties have you encountered?\n",
    "\n",
    "*_Chạy được toàn bộ project code song song trên gpu free của colab .\\_._*\n",
    "\n",
    "*_Lộn tùm lum chiều của data vì đó giờ toàn lưu data matrix theo hàng nối hàng mà thư viện thì để theo cột nối cột_*\n",
    "* Eachmember: What have you learned?\n",
    "\n",
    "*_Học được cách run song song bằng c/c++ với nguồn tài nguyên free .\\_._*\n",
    "* Your team: If you had more time, what would you do?\n",
    "\n",
    "*_Có thể optimise pha nhân image_channel với kernel tương ứng dựa trên share_const memory với data truyền vào (image) vì kích thước của một image tương đối nhỏ_*\n",
    "\n",
    "*_Tìm hiểu sâu hơn về cơ chế lưu trữ data khi chuyển đổi kích thước trong Eigen để tiến hành thực hiện pha song song hóa cho toàn bộ batch truyền vào trong forward-conv-layer_*\n",
    "\n",
    "* Mong muốn cá nhân của nhóm\n",
    "\n",
    "*_Trước hết em xin nói đây là một ý tưởng cho project tuyệt vời! Tuy nhiên thời gian của tụi em phần lớn ko nằm ở việc optimise mà lại nằm ở setup làm sao cho nó chạy được và test được với những gì tụi em làm. Nếu như thầy còn tiếp tục tận dụng project này cho các khóa sau thì em mong muốn thầy có thể set up được một môi trường sẵn và template sẵn (như các bài lab) để tụi em có thể tiến hành đi sâu hơn về kỹ thuật optimise của môn. Tụi em ko nghĩ là template của tụi em đủ đẹp để tái sử dụng nhưng cũng tạm coi là ổn để thầy có thể tận dụng và build ra template như những bài lab. Tụi em cảm ơn thầy vì môn học!_*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyqtenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
